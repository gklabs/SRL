sentence of max length is  20
(93354, 13)
sentence of max length is  19
(55317, 13)
shapes of train validation sets
(74683, 13)
(18671, 13)
vocab length is  5171
Created weights matrix from pre trained that would act as a basis for lookup table
vocab length is  2063
Created weights matrix from pre trained that would act as a basis for lookup table
vocab length is  4598
Created weights matrix from pre trained that would act as a basis for lookup table
<class 'numpy.ndarray'>
(74683, 50)
<class 'scipy.sparse.csr.csr_matrix'>
(74683, 180)
59
Label encoder classes- ['(A0*', '(A0*)', '(A1*', '(A1*)', '(A2*', '(A2*)', '(A3*', '(A3*)', '(A4*', '(A4*)', '(A5*)', '(AM-ADV*', '(AM-ADV*)', '(AM-CAU*', '(AM-DIR*', '(AM-DIR*)', '(AM-DIS*', '(AM-DIS*)', '(AM-EXT*', '(AM-EXT*)', '(AM-LOC*', '(AM-LOC*)', '(AM-MNR*', '(AM-MNR*)', '(AM-MOD*', '(AM-MOD*)', '(AM-NEG*)', '(AM-PNC*', '(AM-PRD*', '(AM-PRD*)', '(AM-TMP*', '(AM-TMP*)', '(C-A0*', '(C-A0*)', '(C-A1*', '(C-A1*)', '(C-A2*', '(C-A3*', '(C-V*)', '(R-A0*', '(R-A0*)', '(R-A1*', '(R-A1*)', '(R-A2*', '(R-A2*)', '(R-A3*)', '(R-A4*', '(R-AM-ADV*', '(R-AM-CAU*)', '(R-AM-EXT*', '(R-AM-LOC*', '(R-AM-LOC*)', '(R-AM-MNR*', '(R-AM-MNR*)', '(R-AM-TMP*)', '(V*', '(V*)', '*', '*)', '0', 'other']
	 Model Summary:
	  FFNN(
  (fc1): Linear(in_features=230, out_features=20, bias=True)
  (fc2): Linear(in_features=20, out_features=20, bias=True)
  (fc3): Linear(in_features=20, out_features=60, bias=True)
  (sigmoid): Sigmoid()
  (softmax): Softmax(dim=None)
)
	 Epoch 1/100 	 loss=4.0942 	 val_loss=4.0940 	 time=5.15s
	 Epoch 2/100 	 loss=4.0935 	 val_loss=4.0932 	 time=10.61s
	 Epoch 3/100 	 loss=4.0927 	 val_loss=4.0923 	 time=21.18s
	 Epoch 4/100 	 loss=4.0918 	 val_loss=4.0914 	 time=32.00s
	 Epoch 5/100 	 loss=4.0908 	 val_loss=4.0902 	 time=37.54s
	 Epoch 6/100 	 loss=4.0896 	 val_loss=4.0890 	 time=41.69s
	 Epoch 7/100 	 loss=4.0883 	 val_loss=4.0875 	 time=46.19s
	 Epoch 8/100 	 loss=4.0867 	 val_loss=4.0858 	 time=50.21s
	 Epoch 9/100 	 loss=4.0849 	 val_loss=4.0837 	 time=54.04s
	 Epoch 10/100 	 loss=4.0826 	 val_loss=4.0812 	 time=58.09s
	 Epoch 11/100 	 loss=4.0799 	 val_loss=4.0781 	 time=61.67s
	 Epoch 12/100 	 loss=4.0764 	 val_loss=4.0741 	 time=65.42s
	 Epoch 13/100 	 loss=4.0720 	 val_loss=4.0690 	 time=69.52s
	 Epoch 14/100 	 loss=4.0662 	 val_loss=4.0620 	 time=74.12s
	 Epoch 15/100 	 loss=4.0580 	 val_loss=4.0520 	 time=78.20s
	 Epoch 16/100 	 loss=4.0458 	 val_loss=4.0367 	 time=82.49s
	 Epoch 17/100 	 loss=4.0264 	 val_loss=4.0110 	 time=86.41s
	 Epoch 18/100 	 loss=3.9915 	 val_loss=3.9618 	 time=90.65s
	 Epoch 19/100 	 loss=3.9192 	 val_loss=3.8547 	 time=94.97s
	 Epoch 20/100 	 loss=3.7681 	 val_loss=3.6561 	 time=98.68s
	 Epoch 21/100 	 loss=3.5746 	 val_loss=3.4897 	 time=102.69s
	 Epoch 22/100 	 loss=3.4593 	 val_loss=3.4154 	 time=106.54s
	 Epoch 23/100 	 loss=3.4097 	 val_loss=3.3823 	 time=110.44s
	 Epoch 24/100 	 loss=3.3861 	 val_loss=3.3651 	 time=114.61s
	 Epoch 25/100 	 loss=3.3729 	 val_loss=3.3548 	 time=118.29s
	 Epoch 26/100 	 loss=3.3649 	 val_loss=3.3482 	 time=122.52s
	 Epoch 27/100 	 loss=3.3594 	 val_loss=3.3435 	 time=126.90s
	 Epoch 28/100 	 loss=3.3555 	 val_loss=3.3401 	 time=131.17s
	 Epoch 29/100 	 loss=3.3525 	 val_loss=3.3375 	 time=135.42s
	 Epoch 30/100 	 loss=3.3503 	 val_loss=3.3355 	 time=139.10s
	 Epoch 31/100 	 loss=3.3485 	 val_loss=3.3339 	 time=142.96s
	 Epoch 32/100 	 loss=3.3471 	 val_loss=3.3326 	 time=147.58s
	 Epoch 33/100 	 loss=3.3459 	 val_loss=3.3315 	 time=151.63s
	 Epoch 34/100 	 loss=3.3449 	 val_loss=3.3305 	 time=156.07s
	 Epoch 35/100 	 loss=3.3441 	 val_loss=3.3297 	 time=159.88s
	 Epoch 36/100 	 loss=3.3433 	 val_loss=3.3291 	 time=164.53s
	 Epoch 37/100 	 loss=3.3428 	 val_loss=3.3284 	 time=169.05s
	 Epoch 38/100 	 loss=3.3421 	 val_loss=3.3279 	 time=173.59s
	 Epoch 39/100 	 loss=3.3417 	 val_loss=3.3275 	 time=177.59s
	 Epoch 40/100 	 loss=3.3412 	 val_loss=3.3270 	 time=181.19s
	 Epoch 41/100 	 loss=3.3408 	 val_loss=3.3267 	 time=184.78s
	 Epoch 42/100 	 loss=3.3404 	 val_loss=3.3263 	 time=188.62s
	 Epoch 43/100 	 loss=3.3403 	 val_loss=3.3260 	 time=193.62s
	 Epoch 44/100 	 loss=3.3400 	 val_loss=3.3257 	 time=197.74s
	 Epoch 45/100 	 loss=3.3396 	 val_loss=3.3255 	 time=201.31s
	 Epoch 46/100 	 loss=3.3394 	 val_loss=3.3252 	 time=204.85s
	 Epoch 47/100 	 loss=3.3392 	 val_loss=3.3250 	 time=208.44s
	 Epoch 48/100 	 loss=3.3390 	 val_loss=3.3248 	 time=211.94s
	 Epoch 49/100 	 loss=3.3388 	 val_loss=3.3247 	 time=215.84s
	 Epoch 50/100 	 loss=3.3386 	 val_loss=3.3245 	 time=219.37s
	 Epoch 51/100 	 loss=3.3384 	 val_loss=3.3243 	 time=222.88s
	 Epoch 52/100 	 loss=3.3382 	 val_loss=3.3242 	 time=227.38s
	 Epoch 53/100 	 loss=3.3381 	 val_loss=3.3240 	 time=231.63s
	 Epoch 54/100 	 loss=3.3380 	 val_loss=3.3239 	 time=235.38s
	 Epoch 55/100 	 loss=3.3378 	 val_loss=3.3238 	 time=239.88s
	 Epoch 56/100 	 loss=3.3378 	 val_loss=3.3237 	 time=243.67s
	 Epoch 57/100 	 loss=3.3376 	 val_loss=3.3236 	 time=247.71s
	 Epoch 58/100 	 loss=3.3375 	 val_loss=3.3235 	 time=251.72s
	 Epoch 59/100 	 loss=3.3374 	 val_loss=3.3233 	 time=255.63s
	 Epoch 60/100 	 loss=3.3373 	 val_loss=3.3233 	 time=259.23s
	 Epoch 61/100 	 loss=3.3372 	 val_loss=3.3232 	 time=262.96s
	 Epoch 62/100 	 loss=3.3374 	 val_loss=3.3231 	 time=266.66s
	 Epoch 63/100 	 loss=3.3371 	 val_loss=3.3230 	 time=270.54s
	 Epoch 64/100 	 loss=3.3370 	 val_loss=3.3229 	 time=274.62s
	 Epoch 65/100 	 loss=3.3370 	 val_loss=3.3229 	 time=278.37s
	 Epoch 66/100 	 loss=3.3369 	 val_loss=3.3228 	 time=282.21s
	 Epoch 67/100 	 loss=3.3368 	 val_loss=3.3227 	 time=286.09s
	 Epoch 68/100 	 loss=3.3368 	 val_loss=3.3227 	 time=289.84s
	 Epoch 69/100 	 loss=3.3368 	 val_loss=3.3226 	 time=293.52s
	 Epoch 70/100 	 loss=3.3368 	 val_loss=3.3226 	 time=297.47s
	 Epoch 71/100 	 loss=3.3367 	 val_loss=3.3225 	 time=301.33s
	 Epoch 72/100 	 loss=3.3366 	 val_loss=3.3225 	 time=305.05s
	 Epoch 73/100 	 loss=3.3365 	 val_loss=3.3224 	 time=309.84s
	 Epoch 74/100 	 loss=3.3364 	 val_loss=3.3224 	 time=315.13s
	 Epoch 75/100 	 loss=3.3365 	 val_loss=3.3223 	 time=319.57s
	 Epoch 76/100 	 loss=3.3364 	 val_loss=3.3223 	 time=324.33s
	 Epoch 77/100 	 loss=3.3363 	 val_loss=3.3222 	 time=328.56s
	 Epoch 78/100 	 loss=3.3363 	 val_loss=3.3222 	 time=333.63s
	 Epoch 79/100 	 loss=3.3362 	 val_loss=3.3222 	 time=339.56s
	 Epoch 80/100 	 loss=3.3361 	 val_loss=3.3221 	 time=344.53s
	 Epoch 81/100 	 loss=3.3362 	 val_loss=3.3221 	 time=349.00s
	 Epoch 82/100 	 loss=3.3362 	 val_loss=3.3221 	 time=353.01s
	 Epoch 83/100 	 loss=3.3361 	 val_loss=3.3220 	 time=357.40s
	 Epoch 84/100 	 loss=3.3360 	 val_loss=3.3220 	 time=362.10s
	 Epoch 85/100 	 loss=3.3361 	 val_loss=3.3219 	 time=365.70s
	 Epoch 86/100 	 loss=3.3360 	 val_loss=3.3219 	 time=371.08s
	 Epoch 87/100 	 loss=3.3360 	 val_loss=3.3219 	 time=374.76s
	 Epoch 88/100 	 loss=3.3359 	 val_loss=3.3219 	 time=379.33s
	 Epoch 89/100 	 loss=3.3359 	 val_loss=3.3218 	 time=382.93s
	 Epoch 90/100 	 loss=3.3358 	 val_loss=3.3218 	 time=387.04s
	 Epoch 91/100 	 loss=3.3358 	 val_loss=3.3218 	 time=390.50s
	 Epoch 92/100 	 loss=3.3359 	 val_loss=3.3217 	 time=393.56s
	 Epoch 93/100 	 loss=3.3358 	 val_loss=3.3217 	 time=396.99s
	 Epoch 94/100 	 loss=3.3358 	 val_loss=3.3217 	 time=400.56s
	 Epoch 95/100 	 loss=3.3358 	 val_loss=3.3217 	 time=404.04s
	 Epoch 96/100 	 loss=3.3358 	 val_loss=3.3217 	 time=407.62s
	 Epoch 97/100 	 loss=3.3356 	 val_loss=3.3216 	 time=410.89s
	 Epoch 98/100 	 loss=3.3358 	 val_loss=3.3216 	 time=414.19s
	 Epoch 99/100 	 loss=3.3357 	 val_loss=3.3216 	 time=417.60s
	 Epoch 100/100 	 loss=3.3357 	 val_loss=3.3216 	 time=421.02s
	 Confusion matrix 
 	 [[0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]]
	 Accuracy is 0.77
